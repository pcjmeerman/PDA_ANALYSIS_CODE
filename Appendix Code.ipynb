{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fee8096",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "## Data and Uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350649cf",
   "metadata": {},
   "source": [
    "First obtain a matrix with the information of alternatives on attributes, including the uncertainty ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a50bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools as it\n",
    "from scipy.stats import norm\n",
    "from warnings import warn\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "class Dataset():\n",
    "     \n",
    "    \n",
    "    def __init__(self, dataset, sheetname_matrix):\n",
    "        self.dataset = dataset\n",
    "        self.sheetname_matrix = sheetname_matrix\n",
    "        \n",
    "    def read_dataset(self):\n",
    "        return pd.read_excel(self.dataset, self.sheetname_matrix)\n",
    "\n",
    "    def isolate_alternatives(self):\n",
    "        self.alternatives_array = self.read_dataset().values[:,0].astype(int)\n",
    "        return self.alternatives_array\n",
    "    \n",
    "    def column_names(self):\n",
    "        return list(self.read_dataset().columns)\n",
    "    \n",
    "    def criteria_weights(self, sheetname_criteria = \"Criteria\"):\n",
    "        return pd.read_excel(self.dataset, sheetname_criteria).values\n",
    "    \n",
    "    def uncertainty(self, sheetname_uncertainty = \"Deviation\"):\n",
    "        return pd.read_excel(self.dataset, sheetname_uncertainty)\n",
    "    \n",
    "    def metrics(self, sheetname_metrics =  \"Units\"):\n",
    "        return pd.read_excel(self.dataset, sheetname_metrics)\n",
    "  \n",
    "    def scales_gather(self, sheetname_metrics =  \"Units\"):\n",
    "        return pd.read_excel(self.dataset, sheetname_metrics).iloc[[1,2]]\n",
    "\n",
    "    def interactions(self, sheetname_interactions =  \"Interactions\"):\n",
    "        return pd.read_excel(self.dataset, sheetname_interactions)\n",
    "    \n",
    "Complete_Matrix = Dataset(\"C:/Users/paulu/Documents/Epa/Thesis/Complete_Matrix.xlsx\", sheetname_matrix ='Sheet2')\n",
    "print(\"Data loaded\")\n",
    "\n",
    "Complete_Matrix.criteria_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bc7fc",
   "metadata": {},
   "source": [
    "### Generating Box plot representation of the impacts of the alternatives using Uncertainty_Alternatives Class\n",
    "\n",
    "In order to present the uncertainty of the impacts of an alternative the Class \"Uncertainty_Alternatives()\" has been written.\n",
    "The distribution of the uncertainty can have different forms, but since that information is lacking a normal distribution is assumed. If another form of distribution is used this will be noted.\n",
    "In order to be able to generate the boxplots the corresponding deviations, have to be filled in in the excel file. For this there is a sheetname called \"Deviation\" and on the corresponding places of the means (same position on the grid in the other sheet), the standard deviations have to be filled in. For now often assumed to be 10% of the mean value. \n",
    "\n",
    "From this range a normal distribution is to be generated in the methods of the class setting the numpy.random.seed(0) and the using np.random.normal methods. the outcome is a set of data used to represent the uncertainty in the boxplots. However, if the alternatives generate no impact == 0, on an attribute, for clarity sake, no boxplot is generated. (This would represent just a line on the x-axis).\n",
    "\n",
    "If one alternative on one attribute has to be generated one can use the boxplot() method. Here alt = the alternative to be represented in integer format, n = 150, the amount of random generated data is used for the boxplot and the attribute is the name of the attribute to be represented in string format.\n",
    "\n",
    "The process is the same for the other methods, \"alternatives()\" and \"all_alternatives()\", which present all the alternative on 1 attribute and all alternatives on all attributes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ae2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uncertainty_Alternatives():\n",
    "    def __init__(self, dataset = Complete_Matrix):\n",
    "        self.dataset = dataset\n",
    "        self.uncertainty_df = dataset.uncertainty()\n",
    "        self.alternatives_df = dataset.read_dataset()\n",
    "        self.temp_df = pd.DataFrame()\n",
    "        sns.set(rc={\"figure.figsize\":(7, 5)})\n",
    "        sns.set_style('whitegrid')\n",
    "        \n",
    "                \n",
    "        #If you have filled in the deviations of an alternative in log-normal distributions values you can add these to this list for\n",
    "        #that particulare attribute.\n",
    "        #the code will draw n random numbers from a lognormal distributions using the given mean and sigma from the excell file and not use a normal distribution.\n",
    "        #Be aware that in the alternative_uncertainty method, the mean value is put in a log function; So you don't have to do that in the excell file itself.\n",
    "        #however, instead of the standard deviation in the \"Deviation Tab\" you need to fill in the sigma function.\n",
    "        self.lognormal_list_alt = []\n",
    "        self.uniform_list_alt = []\n",
    "        \n",
    "        return\n",
    "\n",
    "    def boxplot(self, alt, n = 100000, attribute = \"Attribute_3\"):\n",
    "        if alt < 1:\n",
    "            return \"alternatives number must be greater than zero, otherwise things go wrong, trust me, than it starts from the other side of the matrix\"\n",
    "        np.random.seed(0)\n",
    "        if alt not in self.lognormal_list_alt and alt not in self.uniform_list_alt:\n",
    "            print(\"Normal distribution found for alternative:\", str(alt))\n",
    "            Distribution_generation = np.random.normal(loc=self.alternatives_df[attribute].values[alt-1], scale=self.uncertainty_df[attribute].values[alt-1], size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                print(\"Impact of Alternative \", str(self.alternatives_df[\"Name\"].values[alt-1]), \"is zero on the selected attribute\")\n",
    "                return\n",
    "            ax = sns.boxplot(y = Distribution_generation, showfliers = False)\n",
    "            ax.set_title(\"Impact on \" + attribute)\n",
    "            ax.set_xlabel(\"Alternative \" + str(alt) + \": \"+ str(self.alternatives_df[\"Name\"].values[alt-1]));\n",
    "            ax.set_ylabel(self.dataset.metrics()[attribute].values[0])\n",
    "            print(\"Scale = \" + str(self.uncertainty_df[attribute].values[alt-1]))\n",
    "            print(\"Loc = \"+ str(self.alternatives_df[attribute].values[alt-1]))\n",
    "            #ax.set(ylim=(8, 16))\n",
    "            plt.show()\n",
    "        \n",
    "            return\n",
    "        if alt in self.lognormal_list_alt:\n",
    "            #0.03*self.uncertainty_df[attribute].values[alt-1]\n",
    "            self.sigma = 1.2\n",
    "            print(\"Log-normal distribution found for alternative:\", str(alt))\n",
    "            Distribution_generation = np.random.lognormal(mean=math.log(self.alternatives_df[attribute].values[alt-1]), sigma=self.sigma, size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                print(\"Impact of Alternative \", str(self.alternatives_df[\"Name\"].values[alt-1]), \"is zero on the selected attribute\")\n",
    "                return\n",
    "            ax = sns.boxplot(y = Distribution_generation, showfliers = False)\n",
    "            ax.set_title(\"Impact on \" + attribute)\n",
    "            ax.set_xlabel(\"Alternative \" + str(alt) + \": \"+ str(self.alternatives_df[\"Name\"].values[alt-1]));\n",
    "            ax.set_ylabel(self.dataset.metrics()[attribute].values[0])\n",
    "            #ax.set(ylim=(8, 16))\n",
    "            #self.uncertainty_df[attribute].values[alt-1]\n",
    "            print(\"Sigma = \" + str(self.sigma))\n",
    "            print(\"Mean = \"+ str(self.alternatives_df[attribute].values[alt-1]))\n",
    "            plt.show()\n",
    "\n",
    "        if alt in self.uniform_list_alt:\n",
    "            #0.03*self.uncertainty_df[attribute].values[alt-1]\n",
    "            self.low = self.alternatives_df[attribute].values[alt-1] - self.uncertainty_df[attribute].values[alt-1]\n",
    "            self.high = self.alternatives_df[attribute].values[alt-1] + self.uncertainty_df[attribute].values[alt-1]\n",
    "            print(\"Uniform distribution found for alternative:\", str(alt))\n",
    "            Distribution_generation = np.random.uniform(low=self.low, high = self.high, size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                print(\"Impact of Alternative \", str(self.alternatives_df[\"Name\"].values[alt-1]), \"is zero on the selected attribute\")\n",
    "                return\n",
    "            ax = sns.boxplot(y = Distribution_generation, showfliers = False)\n",
    "            ax.set_title(\"Impact on \" + attribute)\n",
    "            ax.set_xlabel(\"Alternative \" + str(alt) + \": \"+ str(self.alternatives_df[\"Name\"].values[alt-1]));\n",
    "            ax.set_ylabel(self.dataset.metrics()[attribute].values[0])\n",
    "            #ax.set(ylim=(8, 16))\n",
    "            #self.uncertainty_df[attribute].values[alt-1]\n",
    "            print(\"Low = \" + str(self.low))\n",
    "            print(\"High = \"+ str(self.high))\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "   \n",
    "    def distributions(self, alt = 3, n = 100000, attribute = \"Attribute_3\"):\n",
    "        if alt < 1:\n",
    "            #print(\"alternatives number must be greater than zero, otherwise things go wrong, trust me, than it starts from the other side of the matrix\")\n",
    "            return \n",
    "        if alt not in self.lognormal_list_alt and alt not in self.uniform_list_alt:\n",
    "            np.random.seed(1)\n",
    "            Distribution_generation = np.random.normal(loc=self.alternatives_df[attribute].values[alt-1],scale=self.uncertainty_df[attribute].values[alt-1], size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                #print(\"Impact of Alternative \" + str(self.alt) + \" \" + str(self.alternatives_df[\"Name\"].values[self.alt-1]), \"is zero\")\n",
    "                return\n",
    "            self.temp_df[\"Alt_\" + str(alt)] = Distribution_generation\n",
    "            return self.temp_df\n",
    "        if alt in self.lognormal_list_alt:\n",
    "            self.sigma = 0.2\n",
    "           # 0.0135*self.uncertainty_df[attribute].values[alt-1]\n",
    "            np.random.seed(1)\n",
    "            Distribution_generation = np.random.lognormal(mean=math.log(self.alternatives_df[attribute].values[alt-1]), sigma= self.sigma, size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                #print(\"Impact of Alternative \" + str(self.alt) + \" \" + str(self.alternatives_df[\"Name\"].values[self.alt-1]), \"is zero\")\n",
    "                return\n",
    "            self.temp_df[\"Alt_\" + str(alt)] = Distribution_generation\n",
    "            return self.temp_df\n",
    "\n",
    "        if alt in self.uniform_list_alt:\n",
    "            self.low = self.alternatives_df[attribute].values[alt-1] - self.uncertainty_df[attribute].values[alt-1]\n",
    "            self.high = self.alternatives_df[attribute].values[alt-1] + self.uncertainty_df[attribute].values[alt-1]\n",
    "           # 0.0135*self.uncertainty_df[attribute].values[alt-1]\n",
    "            #print(\"Uniform distribution found for alternative:\", str(alt))\n",
    "            np.random.seed(1)\n",
    "            Distribution_generation = np.random.uniform(low=self.low, high = self.high, size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                #print(\"Impact of Alternative \" + str(self.alt) + \" \" + str(self.alternatives_df[\"Name\"].values[self.alt-1]), \"is zero\")\n",
    "                return\n",
    "            self.temp_df[\"Alt_\" + str(alt)] = Distribution_generation\n",
    "            return self.temp_df\n",
    "\n",
    "    def alternatives(self, attribute = \"Attribute_3\"):\n",
    "        self.temp_df = pd.DataFrame()\n",
    "        sns.set(rc={\"figure.figsize\":(7, 5.5)})\n",
    "        sns.set_style('whitegrid')\n",
    "        for i in range(self.alternatives_df.values.shape[0]):\n",
    "            \n",
    "            #i becomes to large probably; look at alt = i+1\n",
    "            self.distributions(alt = i+1, attribute = attribute)\n",
    "        ax = sns.boxplot(x = 'variable', y = 'value', data = pd.melt(self.temp_df), showfliers = False, palette = \"Blues\")\n",
    "        ax.set_title(\"Impact on: \" + str(attribute))\n",
    "        ax.set_xlabel(\"Alternatives\")\n",
    "        ax.set_ylabel(self.dataset.metrics()[attribute].values[0])\n",
    "\n",
    "        return plt.show()\n",
    "    \n",
    "    def all_alternatives(self):\n",
    "        for j in range(2, (len(self.dataset.column_names()))):\n",
    "            self.alternatives(attribute = self.dataset.column_names()[j])\n",
    "            plt.show()\n",
    "        return \n",
    "\n",
    "Uncertainty_Data_Alternatives = Uncertainty_Alternatives()\n",
    "#Uncertainty_Data_Alternatives.alternatives_df.values.shape[0]\n",
    "#Uncertainty_Data_Alternatives.boxplot(alt = 4, attribute = 'Chrome_concentration_in_Crops')\n",
    "#Uncertainty_Data_Alternatives.boxplot(alt = 5, attribute = 'Chrome_concentration_in_Crops')\n",
    "#Uncertainty_Data_Alternatives.boxplot(alt = 8, attribute = 'Chrome_concentration_in_Crops')\n",
    "#test = Uncertainty_Data_Alternatives.boxplot(alt = 2, attribute = \"Attribute_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d36b33",
   "metadata": {},
   "source": [
    "### Generate the Solution Space\n",
    "To generate the solution space all alternative combinations are generated. Some alternatives are mutually exclusive or dependent, this information is than also added. For example, alternative 1 is the BUA case and is exclusive with all alternatives. Therefore this alternative is isolated before the solution space (\"def solspace()\") is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explicit_Space():\n",
    "    def __init__(self, choices, alternative_set = Complete_Matrix):\n",
    "        self.n = choices\n",
    "        self.set = alternative_set.isolate_alternatives()\n",
    "        self.subseq = []  # subsequent activities\n",
    "        self.mut_exc = []  # mutually exclusive\n",
    "        self.solution_space = \"Not yet generated\"\n",
    "\n",
    "    def add_mutually_exclusive(self, a, b):\n",
    "\n",
    "        # if a >= self.n:\n",
    "        #    _msj = ('a is out of bounds. Maximum value is {0} and \\\n",
    "        #            got {1}'.format(self.n, a))\n",
    "        #    raise ValueError(_msj)\n",
    "\n",
    "        # if b >= self.n:\n",
    "        #    _msj = ('b is out of bounds. Maximum value is {0} and \\\n",
    "        #            got {1}'.format(self.n, b))\n",
    "        #    raise ValueError(_msj)\n",
    "\n",
    "        _ac = [a, b]\n",
    "        _ca = [b, a]\n",
    "        if _ac not in self.mut_exc or _ca not in self.mut_exc:\n",
    "            self.mut_exc.append(_ac)\n",
    "        else:\n",
    "            warn('Mutualy exclusive activity already added')\n",
    "\n",
    "    def add_subsequent(self, parent, sub):\n",
    "\n",
    "        #if parent >= self.n:\n",
    "         #   _msj = ('parent is out of bounds. Maximum value is {0} and \\\n",
    "         #           got {1}'.format(self.n, parent))\n",
    "          #  raise ValueError(_msj)\n",
    "\n",
    "        #if sub >= self.n:\n",
    "        #    _msj = ('sub is out of bounds. Maximum value is {0} and \\\n",
    "         #           got {1}'.format(self.n, sub))\n",
    "         #   raise ValueError(_msj)\n",
    "\n",
    "        _ac = [parent, sub]\n",
    "        if _ac not in self.subseq:\n",
    "            self.subseq.append(_ac)\n",
    "        else:\n",
    "            warn('Subsequent activity already added')\n",
    "\n",
    "    def generate(self):\n",
    "        for m in range(self.n+1):\n",
    "            for item in it.combinations(self.set, m):\n",
    "                #print(item)\n",
    "                _flag = True\n",
    "                #for si in self.subseq:\n",
    "                  #  if si == si:\n",
    "                        #_flag = False\n",
    "                        #print(_flag)\n",
    "                  #      break\n",
    "                   # print(\"Dependecy found\")\n",
    "            # Check for mutually exclusive\n",
    "            \n",
    "            \n",
    "            \n",
    "                for si in self.subseq:\n",
    "                    if si[0] in item and si[1] not in item:\n",
    "                        _flag = False\n",
    "                        break\n",
    "                    \n",
    "                if _flag:\n",
    "                    for ei in self.mut_exc:\n",
    "                   # print((ei[0], ei[1]))\n",
    "                        if  ei[0] in item and ei[1] in item:\n",
    "                        #print(item)\n",
    "                        #print(ei[0], ei[1])\n",
    "                            _flag = False\n",
    "                            break\n",
    "                        #print(_flag)\n",
    "\n",
    "\n",
    "                if _flag:\n",
    "                    yield item\n",
    "\n",
    "    def solspace(self):\n",
    "        list = []\n",
    "        for item in self.generate():\n",
    "            list.append(item)\n",
    "        self.solution_space = list\n",
    "        return list\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.solspace())\n",
    "    \n",
    "print(\"Explicit Class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641fcb3",
   "metadata": {},
   "source": [
    "### Data Acquisistion\n",
    "From the generated solution space the corresponding information of each alternative on each attribute is collected in the class \"Data_Acquisition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Acquisition():\n",
    "    def __init__(self, dataset = Complete_Matrix, n = 1,):\n",
    "        self.dataset = dataset\n",
    "        self.n = n\n",
    "        self.stop = False\n",
    "        self.collected_data = \"No Data Generated as of yet\"\n",
    "        self.temp = Explicit_Space(self.n, self.dataset)\n",
    "        \n",
    "        #Fill in what mutually exclusive project there are:\n",
    "        #For now, alternative one represents the BAU case.\n",
    "        for i in range(2,self.dataset.alternatives_array.shape[0]):\n",
    "            self.temp.add_mutually_exclusive(1,i)\n",
    "        self.temp.add_mutually_exclusive(2,9)\n",
    "        self.temp.add_mutually_exclusive(3,9)\n",
    "        self.temp.add_mutually_exclusive(4,9)\n",
    "        self.list_combinations = self.temp.solspace()\n",
    "        self.collected_df = \"No Data Generated as of yet\"\n",
    "        self.df_aggregated = \"No Data Generated as of yet\"\n",
    "    \n",
    "    def collect(self):\n",
    "   # array = np.asarray(decision_matrix)\n",
    "    #print(decision_matrix)\n",
    "    #print(list_combinations)\n",
    "        decision_matrix = self.dataset.read_dataset()\n",
    "        list_combinations = self.list_combinations\n",
    "\n",
    "    #First create an empty set in which the different alternatives are stored\n",
    "        Collected_sets = []\n",
    "    #iterate trought the combinations created by the Explicit_Space Class.\n",
    "        for i in list_combinations:\n",
    "            if self.stop == True:\n",
    "                if len(Collected_sets) > self.portfolio_set:\n",
    "                    self.stop = False\n",
    "                    return Collected_sets\n",
    "        #From the original dataset each row with al the data has to be selected/collected.\n",
    "        #For this first an empty set is made to obtain all the information for each combination set and store this one in T_set\n",
    "            T_set = []\n",
    "        #For each element, j, in the ith set of combination the information is obtained. \n",
    "        #If element i = 4, contains the Alternatives, 5, 6, 19 and 22 than for these the information has to be collected\n",
    "            for j in i:\n",
    "            #The information is collected by searching trought the original data set (decision_matrix) by compairing the Alternative numbers\n",
    "            \n",
    "                A_selected = decision_matrix.loc[decision_matrix[self.dataset.column_names()[0]] == j]\n",
    "            #For collection purposes the pandadataframe is converted to a simple list. \n",
    "            #The [0] in the next section is needed to get the list out of the lists.\n",
    "            #To many lists [[[5]]].\n",
    "                A_converted = A_selected.values.tolist()[0]\n",
    "                A_converted[0] = int(A_converted[0])\n",
    "            #The values are appended to the list until all combinations within the ith set are added.\n",
    "                T_set.append(A_converted)\n",
    "            #After this the set is added to the collected set and reset for the next ith combination.\n",
    "            #It is thus a temporary set.\n",
    "        #print(T_set)\n",
    "            Collected_sets.append(T_set)\n",
    "    #print(Collected_sets)\n",
    "        #print(\"For \", self.n, \" action(s) to chooce from the dataset, the combinations are:\")\n",
    "        self.collected_data = Collected_sets\n",
    "        return self.collected_data\n",
    "        \n",
    "    def collect_dataframe(self):\n",
    "        df = []\n",
    "        for i in range (1, self.temp.size()):\n",
    "            test = (self.collected_data[i])\n",
    "            dataframe = pd.DataFrame(test, columns = self.dataset.column_names())\n",
    "            dataframe[\"Portfolio_set\"] = i\n",
    "            dataframe_temp = dataframe\n",
    "            df.append(dataframe_temp)\n",
    "            self.collected_df_seperated = df\n",
    "            self.collected_df = pd.concat(self.collected_df_seperated)\n",
    "        #print(\"For portfolio number\", self.portfolio_set, \"we obtain:\")\n",
    "        self.stop = True\n",
    "        return self.collected_df\n",
    "        #self.collected_df = pd.DataFrame((Collected_sets), columns = self.dataset.column_names())\n",
    "        \n",
    "    \n",
    "    def aggregate(self):\n",
    "        self.sp_temp = []        \n",
    "        df_23=[]\n",
    "        for j in range (1, self.temp.size()):\n",
    "            self.sp = self.collected_df.loc[self.collected_df['Portfolio_set'] == j]\n",
    "            self.sp_temp.append(self.sp)\n",
    "\n",
    "        for i in range(len(self.list_combinations)-1):\n",
    "            result = self.sp_temp[i]\n",
    "            result.pop(\"Portfolio_set\")\n",
    "            result.pop(\"Alternative\")\n",
    "            result.pop(\"Name\")\n",
    "            result_2 = result.sum(axis = 0).to_frame().T\n",
    "            #check wheter there are attributes expressen in percentages in the dataset that have to be calculated differently.\n",
    "            for k in self.sp_temp[i].T.index:\n",
    "                if k == \"Alternative\" or k == \"Name\" or k == \"Portfolio_set\":\n",
    "                    continue\n",
    "                if Complete_Matrix.metrics()[str(k)][0] == \"%\":\n",
    "                    #print(k + \" is expressed in percentages\")\n",
    "                    temp_value = np.asarray(self.sp_temp[i].T.loc[k]/100)\n",
    "                    new_value = temp_value.sum() + np.prod(temp_value)\n",
    "                    result_2[k] = result_2[k].replace(result_2[k][0],new_value*100)\n",
    "            df_23.append(result_2)\n",
    "        self.df_aggregated = pd.concat(df_23)\n",
    "        self.df_aggregated[\"Strategic_Portfolio\"] = list(range(1, self.df_aggregated.shape[0]+1))\n",
    "        for z in self.df_aggregated.columns:\n",
    "            if z == \"Strategic_Portfolio\":\n",
    "                continue\n",
    "            if Complete_Matrix.metrics()[z][1] == 'global':\n",
    "                #print(\"local attribute found: \", z, eval(Complete_Matrix.metrics()[z][2])[1])\n",
    "                self.df_aggregated.loc[self.df_aggregated[z] > eval(Complete_Matrix.metrics()[z][2])[1], z] = eval(Complete_Matrix.metrics()[z][2])[1]\n",
    "                self.df_aggregated.loc[self.df_aggregated[z] < eval(Complete_Matrix.metrics()[z][2])[0], z] = eval(Complete_Matrix.metrics()[z][2])[0]\n",
    "\n",
    "\n",
    "        return self.df_aggregated \n",
    "        return self.df_aggregated          \n",
    "         \n",
    "    \n",
    "    def single_portfolio(self, portfolio_set = 1):\n",
    "        if self.stop == False:\n",
    "            self.collect_dataframe()\n",
    "        selected_portfolio = self.collected_df.loc[self.collected_df['Portfolio_set'] == portfolio_set]\n",
    "        return selected_portfolio          \n",
    "        \n",
    "    def aggregated_single_portfolio(self, portfolio_set = 2):\n",
    "        return self.aggregate_old()[portfolio_set-1:portfolio_set]\n",
    "    \n",
    "\n",
    "    def one_value(self, weight_1 = 0.5 , weight_2 = (1/3), weight_3 = (1/6)):\n",
    "        store=self.aggregate_old()\n",
    "        return pd.DataFrame(store[\"Crit_1\"]*weight_1+store[\"Crit_2\"]*weight_2+store[\"Crit_3\"]*weight_3\n",
    "    , columns = [\"Crit_total\"])\n",
    "    \n",
    "    def run(self):\n",
    "        self.collect()\n",
    "        self.collect_dataframe()\n",
    "        self.aggregate()\n",
    "\n",
    "\n",
    "Portfolio_Data = Data_Acquisition(Complete_Matrix, 5)\n",
    "#Portfolio_Data.dataset\n",
    "Portfolio_Data.run()\n",
    "print(\"Portfolio Data Collected\")\n",
    "\n",
    "#Portfolio_Data.collected_df\n",
    "#Portfolio_Data.single_portfolio(91)\n",
    "#Portfolio_Data.df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd747824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime_plot.drop(runtime_plot.tail(2).index,inplace=True)\n",
    "#sns.lineplot(data = runtime_plot, x = \"Amount of Portfolios\", y = \"Run time (s)\").set(title='Runtime Portfolio Creation [14 Alternatives]')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a60760",
   "metadata": {},
   "source": [
    "### Uncertainty in Portfolios\n",
    "\n",
    "In order to analyse the performances of the portfolios the uncertainty of the performance has to be taken into account.\n",
    "There are two ways to approach this problem, but for both approaches the uncertainty distribution of the alternatives themselves have to be available for the attributes. Before aggregation of the alternative values normalisation of both the values and the uncertainty has to take place.\n",
    "\n",
    "Since a portfolio is a set of alternatives the uncertainties are propogated. One way is to add the uncertainties analytically. The difficulty of this approach is then the situation where uncertainties that are distributed differently have to be combined.\n",
    "The other way is computationally, which is the way taken here. A Monte Carlo simulation is done for the portfolios. This encompasses selecting a value within each distribution of each alternative. These uncertainty distributions have many forms.\n",
    "Then these, for example 5 values are then aggregated as the portfolio value. This process is then iterated many times to approach the uncertainty distribution of the portfolio. From these aggregated values the boxplot performance of a portfolio is then presented.\n",
    "\n",
    "Graphically presenting the performances of all portfolios simultaniously is difficulty since these will be 257 portfolios to be shown. So from the analysis best performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uncertainty_Portfolios():\n",
    "    def __init__(self, dataset_portfolio = Portfolio_Data, dataset_alternatives = Complete_Matrix):\n",
    "        self.dataset = dataset_portfolio\n",
    "        self.uncertainty_df = dataset_alternatives.uncertainty()\n",
    "        self.alternatives_df = dataset_alternatives.read_dataset()\n",
    "        self.dataset_alternatives_col_names = dataset_alternatives.column_names()\n",
    "        self.temp_df = pd.DataFrame()\n",
    "        sns.set(rc={\"figure.figsize\":(4, 4)})\n",
    "        sns.set_style('whitegrid')\n",
    "        self.list_combination = dataset_portfolio.list_combinations\n",
    "        self.temp_df_2 = []\n",
    "        self.attribute = \"not defined\"\n",
    "        \n",
    "        #If you have filled in the deviations of an alternative in log-normal distributions values you can add these to this list for\n",
    "        #that particulare attribute.\n",
    "        #the code will draw n random numbers from a lognormal distributions using the given mean and sigma from the excell file and not use a normal distribution.\n",
    "        #Be aware that in the alternative_uncertainty method, the mean value is put in a log function; So you don't have to do that in the excell file itself.\n",
    "        #however, instead of the standard deviation in the \"Deviation Tab\" you need to fill in the sigma function.\n",
    "        self.lognormal_list_alt = []\n",
    "        self.uniform_list_alt = []\n",
    "        \n",
    "    def alternative_uncertainty(self, attribute, alt, n =100000):\n",
    "        np.random.seed(0)\n",
    "        if alt not in self.lognormal_list_alt and alt not in self.uniform_list_alt:\n",
    "            np.random.seed(1)\n",
    "            Distribution_generation = np.random.normal(loc=self.alternatives_df[attribute].values[alt-1], scale=self.uncertainty_df[attribute].values[alt-1], size = n)\n",
    "            self.temp_df[str(alt)] = Distribution_generation\n",
    "            return self.temp_df\n",
    "        if alt in self.lognormal_list_alt:\n",
    "            self.sigma = 0.2\n",
    "            print(\"log-normal distribution found for alternative:\", str(alt))\n",
    "            np.random.seed(1)\n",
    "            Distribution_generation = np.random.lognormal(mean=math.log(self.alternatives_df[attribute].values[alt-1]), sigma=self.sigma, size = n)\n",
    "            self.temp_df[str(alt)] = Distribution_generation\n",
    "            return self.temp_df\n",
    "        \n",
    "        if alt in self.uniform_list_alt:\n",
    "            print(\"log-normal distribution found for alternative:\", str(alt))\n",
    "            self.low = self.alternatives_df[attribute].values[alt-1] - self.uncertainty_df[attribute].values[alt-1]\n",
    "            self.high = self.alternatives_df[attribute].values[alt-1] + self.uncertainty_df[attribute].values[alt-1]\n",
    "           # 0.0135*self.uncertainty_df[attribute].values[alt-1]\n",
    "            #print(\"Uniform distribution found for alternative:\", str(alt))\n",
    "            np.random.seed(1)\n",
    "            Distribution_generation = np.random.uniform(low=self.low, high = self.high, size = n)\n",
    "            if np.sum(Distribution_generation) == 0.0:\n",
    "                #print(\"Impact of Alternative \" + str(self.alt) + \" \" + str(self.alternatives_df[\"Name\"].values[self.alt-1]), \"is zero\")\n",
    "                return\n",
    "            self.temp_df[\"Alt_\" + str(alt)] = Distribution_generation\n",
    "            return self.temp_df\n",
    "    \n",
    "    def portfolio_uncertainty(self, attribute = \"Time_underperforming_due_to_electricity_shortages\", n = 100000, portfolio_number = 5):\n",
    "        #instead of the current list a portfolio has to be added\n",
    "        np.random.seed(0)\n",
    "        self.temp_df = pd.DataFrame()\n",
    "        for i in self.list_combination[portfolio_number]:\n",
    "            self.alternative_uncertainty(attribute = attribute, alt = i, n = n)\n",
    "        Portfolio_data_boxplot = self.temp_df.values.sum(1)\n",
    "        self.portfolio = pd.DataFrame(Portfolio_data_boxplot, columns = [\"Portfolio number: \" + str(portfolio_number)])\n",
    "        return self.portfolio\n",
    "    \n",
    "    def plot_portfolio(self, portfolio_number = 1, attribute = \"Time_underperforming_due_to_electricity_shortages\", n = 100000):\n",
    "        if portfolio_number > (len(self.list_combination)-1):\n",
    "            print(\"The requested Portfolio does not exist in this dataset, look at how many portfolios you have generated:\" + str(len(Portfolio_Data.list_combinations)-1))\n",
    "            return\n",
    "        self.portfolio_uncertainty(portfolio_number = portfolio_number, attribute = attribute, n = n)\n",
    "        ax = sns.boxplot(x = 'variable', y = 'value', data = pd.melt(self.portfolio), showfliers = False, palette = \"Greens\")\n",
    "        ax.set_title(\"Impact on: \" + str(attribute))\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_ylabel(self.dataset.dataset.metrics()[attribute].values[0])\n",
    "    #Fist obtain the portfolio combinations via explicit space,\n",
    "    \n",
    "    def plot_all_portfolios(self, attribute = \"Time_underperforming_due_to_electricity_shortages\", n = 100000, exclude = True, plot = True):\n",
    "        self.attribute = attribute\n",
    "        sns.set(rc={\"figure.figsize\":(11, 25)})\n",
    "        sns.set_style('whitegrid')\n",
    "        if len(self.temp_df_2) != 0:\n",
    "            print(\"Distributions already created\")\n",
    "        else:\n",
    "            self.temp_df_2 = pd.DataFrame()\n",
    "            for i in range(1, len(self.list_combination)):\n",
    "                if exclude == True:\n",
    "                    if self.portfolio_uncertainty(attribute = attribute, n = n, portfolio_number = i).values.sum(1)[0] == 0.0:\n",
    "                        continue\n",
    "                self.temp_df_2[str(i)]=(self.portfolio_uncertainty(attribute = attribute, n = n, portfolio_number = i)[\"Portfolio number: \" + str(i)])\n",
    "            #if i == 15:\n",
    "                #break\n",
    "        if plot == True:\n",
    "            ax = sns.boxplot(x = 'variable', y = 'value', data = pd.melt(self.temp_df_2), showfliers = False)\n",
    "            ax.set_title(\"Impact on: \" + str(attribute))\n",
    "            ax.set_xlabel(\"Portfolios\")\n",
    "            ax.set_ylabel(self.dataset.dataset.metrics()[attribute].values[0])\n",
    "        return\n",
    "    \n",
    "    def plot_all_portfolios_all_attributes(self, n = 100000):\n",
    "        for j in range(2, (len(self.dataset_alternatives_col_names))):\n",
    "            self.plot_all_portfolios(attribute = self.dataset_alternatives_col_names[j])\n",
    "            plt.show()\n",
    "        return \n",
    "    \n",
    "    def plot_core_portfolios(self, n = 100000, core = [3, 5], attribute = \"Time_underperforming_due_to_electricity_shortages\"):\n",
    "        sns.set(rc={\"figure.figsize\":(11, 7)})\n",
    "        sns.set_style('whitegrid')\n",
    "        self.temp_df_core = pd.DataFrame()\n",
    "        for i in core:\n",
    "            if self.portfolio_uncertainty(attribute = attribute, n = n, portfolio_number = i).values.sum(1)[0] == 0.0:\n",
    "                continue\n",
    "            self.temp_df_core[str(i)]=(self.portfolio_uncertainty(attribute = attribute, n = n, portfolio_number = i)[\"Portfolio number: \" + str(i)])\n",
    "            #if i == 15:\n",
    "                #break\n",
    "        ax = sns.boxplot(x = 'variable', y = 'value', data = pd.melt(self.temp_df_core), showfliers = False)\n",
    "        ax.set_title(\"Impact on: \" + str(attribute))\n",
    "        ax.set_xlabel(\"Portfolios\")\n",
    "        ax.set_ylabel(self.dataset.dataset.metrics()[attribute].values[0])\n",
    "        return\n",
    "    \n",
    "    def plot_core_portfolio_all_attributes(self, core = [3, 5] ):\n",
    "        for j in range(2, (len(self.dataset_alternatives_col_names))):\n",
    "            self.plot_core_portfolios(core = core, attribute = self.dataset_alternatives_col_names[j])\n",
    "            plt.show()\n",
    "        return \n",
    "    \n",
    "    def plot_one_portfolio_all_attributes(self, n = 100000, portfolio_number = 2):\n",
    "        for j in range(2, (len(self.dataset_alternatives_col_names))):\n",
    "            self.plot_portfolio(portfolio_number = portfolio_number, attribute = self.dataset_alternatives_col_names[j])\n",
    "            plt.show()\n",
    "\n",
    "Boxplots = Uncertainty_Portfolios()\n",
    "#Boxplots.plot_core_portfolios(core = Portfolios_with_Synergy.portfolios_with_synergy[\"Strategic_Portfolio\"].values)\n",
    "#Boxplots.plot_core_portfolio_all_attributes(core = Portfolios_with_Synergy.portfolios_with_synergy[\"Strategic_Portfolio\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30017c62",
   "metadata": {},
   "source": [
    "## Value Functions\n",
    "From the obtained portfolio the mean performances are converted to values:\n",
    "The first step to obtain these values is to create a value function. The value function presented here are linear value functions per attribute. The scales are either local or globally determined, depending on the relevant attribute.\n",
    "Secondly, the value functions are created: The portfolios generate a corresponding value per partial value function.\n",
    "\n",
    "The third step is to aggregate the values of each partial value functiond.\n",
    "The fourth is to do this for the portfolio uncertainty distribution generated in the Portfolio_Uncertainty class to obtain the PDF of the values per attribute and then per aggregated portfolio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Values():\n",
    "    def __init__(self, dataset = Portfolio_Data, dataset_2 = Complete_Matrix):\n",
    "        self.dataset_2 = dataset_2\n",
    "        self.portfolio_data = dataset\n",
    "        self.dataset = dataset.df_aggregated\n",
    "        #\n",
    "        #Indexes of the criteria that are deemed 'cost criteria'\n",
    "        #For days attribute the global scales are minimal 0 days, maximal 365 days.\n",
    "        #Per attribute these have to be defined in the excell file.\n",
    "        self.scl = []\n",
    "        self.attribute = \"Profits_from_hides_processing\"\n",
    "        self.info = self.portfolio_data.dataset.scales_gather()\n",
    "    def scales(self):\n",
    "        #from the excell file the scale information is loaded:\n",
    "        #If the scales are globally identified they are obtained from the given input in the Excel.\n",
    "        if self.info[self.attribute].iloc[0] == \"global\":\n",
    "            print(\"A global scale has been identified for attribute: \" + self.attribute) \n",
    "            self.scl = eval(self.dataset_2.scales_gather()[self.attribute].iloc[1])\n",
    "        #If it local variable the min and max are obtained from the Portfolio_Dataset to obtain the ranges.\n",
    "        #Since the ranges are dealing with point data and the portfolio sets have a PDF the highest and lowest found datapoints\n",
    "        #from the Monte Carlo simulations are taken as min and max values.\n",
    "        else:\n",
    "            print(\"A Local scale has been identified for attribute: \" + self.attribute)\n",
    "            Boxplots_values = Uncertainty_Portfolios()\n",
    "            Boxplots_values.plot_all_portfolios(self.attribute, exclude = False, plot = False)\n",
    "            self.temp = Boxplots_values.temp_df_2\n",
    "            self.scl = [np.min(Boxplots_values.temp_df_2.values), np.max(Boxplots_values.temp_df_2.values)]\n",
    "            \n",
    "        return \n",
    "        \n",
    "    \n",
    "    def value_function(self, attribute = \"Space_requirement_for_Waste_Water_Treatment\", p = 10):\n",
    "        self.a_type = self.dataset_2.criteria_weights()[1][self.info.columns.tolist().index(attribute)]\n",
    "        #check wheter new scales have to be loaded for the Value class when new attribute is chocen.\n",
    "        if attribute != self.attribute:\n",
    "            self.attribute = attribute\n",
    "            self.scales()\n",
    "        #first test wheter the scales are loaded, if not load them in via the scales method.\n",
    "        #if len(self.scl) == 0:\n",
    "        #print('Scales are loaded')\n",
    "        #For performance p for p element of self.scale:\n",
    "        #For linear function\n",
    "        if p< min(self.scl) or p>max(self.scl):\n",
    "            print(\"Performance value, p, exceeds value function scale: p =\", str(p), \"when range is\"+str(self.scl)), \n",
    "            return\n",
    "        if self.a_type == \"cost\" or self.a_type == \"Cost\":\n",
    "            return 1-(1/(max(self.scl)-min(self.scl)))*p+((min(self.scl)*-1)/((max(self.scl)-min(self.scl)))), self.a_type\n",
    "        else:\n",
    "            return (1/(max(self.scl)-min(self.scl)))*p+((min(self.scl)*-1)/((max(self.scl)-min(self.scl)))), self.a_type\n",
    "        \n",
    "    \n",
    "    \n",
    "    def Single_Portfolio_Single_Attribute(self, attribute = \"Space_requirement_for_Waste_Water_Treatment\", Portfolio = 1, a_type = \"benefit\",  plot = True):\n",
    "        #Now we are using the value function created to calculate the portfolio value distribution (PDF)\n",
    "        #For this we have to generate the portfolio_distribution corresponding first using the uncertainty class\n",
    "        #If the scales are obtained locally this has already been done and the flag is set to True.\n",
    "        #if the scales are obtained globally the distribution still has to be generated.\n",
    "        self.a_type = self.dataset_2.criteria_weights()[1][self.info.columns.tolist().index(attribute)]\n",
    "        if attribute != self.attribute:\n",
    "            self.attribute = attribute\n",
    "            print(\"Uncertainty Distributions are first generated\")\n",
    "            Boxplots_values = Uncertainty_Portfolios()\n",
    "            Boxplots_values.plot_all_portfolios(self.attribute, exclude = False, plot = False)\n",
    "            self.scl = [np.min(Boxplots_values.temp_df_2.values), np.max(Boxplots_values.temp_df_2.values)]\n",
    "            self.temp = Boxplots_values.temp_df_2\n",
    "        if self.a_type == \"cost\" or self.a_type == \"Cost\":\n",
    "            self.df = 1-((1/(max(self.scl)-min(self.scl))*self.temp[str(Portfolio)].values+(min(self.scl)*-1)/((max(self.scl)-min(self.scl)))))\n",
    "        else: \n",
    "            self.df = 1/(max(self.scl)-min(self.scl))*self.temp[str(Portfolio)].values+(min(self.scl)*-1)/((max(self.scl)-min(self.scl))) \n",
    "        if plot == True:\n",
    "            ax = sns.boxplot(data = self.df, showfliers = False)\n",
    "            ax.set_title(\"Impact on: \" + str(attribute))\n",
    "            ax.set_xlabel(\"Portfolio:\"+str(Portfolio))\n",
    "            ax.set_ylabel(\"Value\")\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def collect_attributes_portfolios(self, attribute = \"Space_requirement_for_Waste_Water_Treatment\", a_type = \"benefit\"):\n",
    "        self.Portfolios_n = np.linspace(1,self.dataset.values.shape[0], self.dataset.values.shape[0])\n",
    "        self.a_type = self.dataset_2.criteria_weights()[1][self.info.columns.tolist().index(attribute)]\n",
    "        if attribute != self.attribute:\n",
    "            self.attribute = attribute\n",
    "            #print(\"Uncertainty Distributions are first generated\")\n",
    "            Boxplots_values = Uncertainty_Portfolios()\n",
    "            Boxplots_values.plot_all_portfolios(self.attribute, exclude = False, plot = False)\n",
    "            self.scl = [np.min(Boxplots_values.temp_df_2.values), np.max(Boxplots_values.temp_df_2.values)]\n",
    "            self.temp = Boxplots_values.temp_df_2\n",
    "        if self.a_type == \"cost\" or self.a_type == \"Cost\":\n",
    "            self.df_all = 1-((1/(max(self.scl)-min(self.scl))*self.temp.values+(min(self.scl)*-1)/((max(self.scl)-min(self.scl)))))\n",
    "        else: \n",
    "            self.df_all = 1/(max(self.scl)-min(self.scl))*self.temp.values+(min(self.scl)*-1)/((max(self.scl)-min(self.scl)))\n",
    "        return self.df_all\n",
    "    \n",
    "    #Method that generates the ranking for cardinal weights w.\n",
    "    \n",
    "    def all_attributes(self):\n",
    "        self.summed_over_attributes = 0\n",
    "        self.generated_attribute_stored = []\n",
    "        weight = self.portfolio_data.dataset.criteria_weights()[0]\n",
    "        for attribute in self.info.columns:\n",
    "            w = weight[self.info.columns.tolist().index(attribute)]\n",
    "            print(attribute + \" with weight:\" + str(w))\n",
    "            #add weights\n",
    "            self.generated_attribute = self.collect_attributes_portfolios(attribute)\n",
    "            self.generated_attribute_stored.append(self.generated_attribute)\n",
    "            #This one immidiatly rewrites and regenerates the analysis. But xi is not stored.\n",
    "            self.summed_over_attributes += (self.generated_attribute*w)\n",
    "        self.F = np.asarray(self.generated_attribute_stored)    \n",
    "        self.portfolio_values_df = pd.DataFrame(self.summed_over_attributes, columns = list(range(1,self.dataset.values.shape[0]+1)))\n",
    "        return self.portfolio_values_df\n",
    "    \n",
    "    #Method to create the entire value dataset.\n",
    "    \n",
    "    def xi(self):\n",
    "        self.generated_attribute_stored = []\n",
    "        for attribute in self.info.columns:\n",
    "            #print(attribute)\n",
    "            #add weights\n",
    "            self.generated_attribute = self.collect_attributes_portfolios(attribute)\n",
    "            self.generated_attribute_stored.append(self.generated_attribute)\n",
    "        self.F = np.asarray(self.generated_attribute_stored)\n",
    "        print(\"Finished\")\n",
    "        return\n",
    "    \n",
    "    #method all_attributes_2 is quicker than the first version because it does not recreate all the uncertainties unncessary.\n",
    "    #Furterhmore is immidiatly converts to the means of the portfolios on the attributes.\n",
    "    def all_attributes_2(self, weight = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])):\n",
    "        np.random.seed(9)\n",
    "        self.summed_over_attributes_2 = 0\n",
    "        #weight = self.portfolio_data.dataset.criteria_weights()[0]\n",
    "        self.weight = weight\n",
    "        for attribute in self.info.columns:\n",
    "            w = self.weight[self.info.columns.tolist().index(attribute)]\n",
    "            #print(attribute + \" with weight:\" + str(w))\n",
    "            self.summed_over_attributes_2 += np.mean(self.F[self.info.columns.tolist().index(attribute)], axis = 0)*w\n",
    "            #self.portfolio_values_df_2 = pd.DataFrame([self.summed_over_attributes_2], columns = list(range(1,self.dataset.values.shape[0]+1)))\n",
    "        return self.summed_over_attributes_2\n",
    "            \n",
    "    \n",
    "    def rank_portfolios_average(self):\n",
    "        #Mean is the expected value from a distribution of discrete numbers.\n",
    "        self.sorted_series =  pd.DataFrame([self.summed_over_attributes_2], columns = list(range(1,self.dataset.values.shape[0]+1))).mean().sort_values(ascending = False).index\n",
    "        return self.sorted_series.values\n",
    "    \n",
    "    def ranks(self, samples = 5):\n",
    "        self.r_1 = []\n",
    "        weight = self.sample_weight_space(samples)\n",
    "        for i in weight:\n",
    "            self.r_1.append(pd.DataFrame([self.all_attributes_2(weight = i)], columns = list(range(1,self.dataset.values.shape[0]+1))).mean().sort_values(ascending = False).index)\n",
    "        self.r_1 = np.asarray(self.r_1)\n",
    "        self.r_1 = np.transpose(self.r_1)\n",
    "        return self.r_1[0]    \n",
    "\n",
    "    def sample_weight_space(self, samples, n=10):\n",
    "        a = np.linspace(np.append(np.insert(arr = np.sort(np.random.uniform(size = n)), obj=0, values = [0]), 1), np.append(np.insert(arr = np.sort(np.random.uniform(size = n)), obj=0, values = [0]), 1), samples)*-1\n",
    "        return np.sort(np.diff(a[::-1]))*-1\n",
    "    \n",
    "    def ranks_2(self, samples = 5):\n",
    "        start = time.time()\n",
    "        self.samples = samples\n",
    "        np.random.seed(9)\n",
    "        self.summed_over_attributes_2 =  0\n",
    "        weight = self.sample_weight_space(samples)\n",
    "        self.matrix = 0\n",
    "        self.dsb = {}\n",
    "        self.r_1 = []\n",
    "        self.weight_rank = self.info.columns.values[np.argsort(self.dataset_2.criteria_weights()[0])]\n",
    "\n",
    "        for attribute in self.weight_rank:\n",
    "            #self.summed_over_attributes_2 += np.mean(self.F[self.info.columns.tolist().index(attribute)], axis = 0)\n",
    "            #Takes the mean of all 137 portfolios on that attribute. This one do want to multiply vectorised before continuing to the next attribute\n",
    "            #Now it recalculates and resums everything unnecissarily.\n",
    "            #matrix is the output of the portfolios on the rows, and the sample of the weight vector per colums.\n",
    "            #This matrix is for the attribute and thus still has to be summed with the other attribute matrices.\n",
    "            #This produces a matrix of values for each portfolio on the rows and sampled weights.\n",
    "            #From this the ranking can occur much faster than iterate over each weight vector.\n",
    "            \n",
    "            \n",
    "            #last part here ignores the weights : \"self.info.columns.tolist().index(attribute)\"\n",
    "            self.matrix += np.outer(np.mean(self.F[self.info.columns.tolist().index(attribute)], axis = 0), weight.T[self.info.columns.values[np.argsort(self.dataset_2.criteria_weights()[0])].tolist().index(attribute)])\n",
    "       #p_vector is the sorted row.\n",
    "        #compare index values to match a portfolio from the matrix index to rank index. \n",
    "        self.matrix_sorted = np.argsort(self.matrix, axis = 0)[::-1]+1\n",
    "        #self.r_1 = pd.DataFrame(matrix_sorted.T, columns = list(range(1,self.dataset.values.shape[0]+1)))\n",
    "        #matrix.sort(axis = 0)\n",
    "        unique, counts = np.unique(self.matrix_sorted[1], return_counts=True)\n",
    "        self.dsa = dict(zip(unique, counts))\n",
    "\n",
    "        for i in range(1, self.matrix_sorted.shape[0]+1):\n",
    "            unique, counts = np.unique(self.matrix_sorted[i-1], return_counts=True)\n",
    "            self.dsa = dict(zip(unique, counts))\n",
    "            self.dsb[\"rank \" + str(i)] = self.dsa\n",
    "\n",
    "        #self.test = unique, counts\n",
    "       # print(self.dsa)     \n",
    "        end = time.time()\n",
    "        elaped_time = end-start \n",
    "        print(\"Elaped time was: \", elaped_time, \" for: \", samples, 'samples')\n",
    "        return\n",
    "    \n",
    "    def acceptability_index(self, rank = \"rank 2\"):\n",
    "        self.dsc = {}\n",
    "        for x in self.dsb[rank]:\n",
    "            #print(\"acceptability for\", rank, \"and portfolio\", x, \"is \", self.dsb[rank][x]/self.samples)\n",
    "            self.dsc[x] = {self.dsb[rank][x]/self.samples}\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def acceptability_indeces(self, amount_of_ranks_included = 10, port_amount = 10):\n",
    "        self.dsx = {}\n",
    "        for j in list(self.dsb.keys()):\n",
    "            self.acceptability_index(rank = j)\n",
    "            self.dsx[j] = self.dsc\n",
    "        list_best = []\n",
    "        \n",
    "\n",
    "        for i in range(1,amount_of_ranks_included+1):\n",
    "            list_best.append(list(self.dsx['rank ' + str(i)].keys()))\n",
    "        flat_list = list(it.chain(*list_best))\n",
    "        self.best = pd.DataFrame(pd.unique(np.asarray(flat_list)), columns = [\"Strategic_Portfolio\"])[0:port_amount]     \n",
    "        return self.best\n",
    "    \n",
    "    def dict_df(self):\n",
    "        self.acceptability = pd.DataFrame(columns = [\"Strategic Portfolio\", \"Acceptibility Index\", \"rank\"])\n",
    "        for i in list(self.dsx.keys()):\n",
    "            regular_list = list(list(self.dsx[i][k]) for (k) in self.dsx[i].keys())\n",
    "            flat_index = [item for sublist in regular_list for item in sublist]\n",
    "            df = pd.DataFrame(list(self.dsx[i].keys()), columns = [\"Strategic Portfolio\"])\n",
    "            df[\"Acceptibility Index\"] = flat_index\n",
    "            df[\"rank\"] = i\n",
    "            self.acceptability = pd.concat([self.acceptability, df])\n",
    "        return self.acceptability\n",
    "\n",
    "    \n",
    "    \n",
    "    #Attribute = on which the portfolios are ranked as best performing.\n",
    "    #Amount = How many of top performing are returned, maximum would be the amount of portfolios generated ranked from low to high.\n",
    "    def heuristic(self, attribute = 'Not Specified', amount_of_ranks_included = 10):\\\n",
    "        #looks at how many attributes there are and uses this lenght to slice the appropiate columns, the other columns are not attributes but name and such.\n",
    "        length = np.asarray(self.dataset_2.column_names()).shape[0]\n",
    "        attribute_array = np.asarray(self.dataset_2.column_names())[2:length]\n",
    "        #Find the corresponding index position for the attribute.\n",
    "#         if attribute == \"Not Specified\":\n",
    "#             attribute = 1\n",
    "        self.index_value = np.where(attribute_array == attribute)\n",
    "        #plus one is to count the first portfolio as portfolio 1 and not as portfolio 0\n",
    "        self.heuristic_outcome = pd.DataFrame(self.F.mean(axis = 1)[self.index_value].argsort().T+1, columns = [\"Portfolios\"]).iloc[-amount_of_ranks_included:][::-1]\n",
    "        self.heuristic_outcome[\"Rank\"] = list(range(1,amount_of_ranks_included +1))\n",
    "        self.heuristic_outcome.set_index(\"Rank\", inplace = True)\n",
    "        return self.heuristic_outcome\n",
    "    \n",
    "    \n",
    "    def return_info_portfolio(self, portfolio = 1):\n",
    "        return self.portfolio_data.single_portfolio(portfolio)\n",
    "    \n",
    "    def return_info_partial_values(self):\n",
    "        self.partial_values = pd.DataFrame(self.F.mean(axis = 1).T, columns= [\"Attribute 1\", \"Attribute 2\", \"Attribute 3\", \"Attribute 4\", \"Attribute 5\", \"Attribute 6\", \"Attribute 7\", \"Attribute 8\", \"Attribute 9\", \"Attribute 10\", \"Attribute 11\"])\n",
    "        return self.partial_values\n",
    "    \n",
    "    def core(self, actor = \"Not Specified\"):\n",
    "        core_array = []\n",
    "        for i in self.best.values.flatten():\n",
    "            core_array.append(self.return_info_portfolio(i)[\"Alternative\"].to_list())\n",
    "        core_array_flat = list(it.chain(*core_array))\n",
    "        core_df = pd.DataFrame(core_array_flat, columns = [\"Core Alternative\"])\n",
    "        self.core_index = core_df.value_counts().to_frame(name = \"Alternative Count\")\n",
    "        self.core_index[\"Actor\"] = actor\n",
    "        self.core_index[\"Core Index\"] = self.core_index.values[:, 0]/self.best.shape[0]\n",
    "        self.core_index.to_csv(\"C:/Users/paulu/Documents/Epa/Thesis/Portfolios \" + str(actor) + \".csv\")\n",
    "        Core_actor = pd.read_csv(\"C:/Users/paulu/Documents/Epa/Thesis/Portfolios \" + str(actor) + \".csv\")\n",
    "        self.CI = Core_actor[[\"Core Alternative\", \"Core Index\"]].rename(columns={\"Core Index\": \"Actor \" + str(actor)})\n",
    "        self.CI.to_csv(\"C:/Users/paulu/Documents/Epa/Thesis/CI \" + str(actor) + \".csv\", index = False)\n",
    "        return self.core_index\n",
    "    \n",
    "    #When wanting to search for all the portfolio that contain specific alternatives one can run this search_alt method.\n",
    "    #Additional ques can be set in the if statement if more alternatives have to be included.\n",
    "    #The mehod returns a list with all the portfolio containing alt_1 and alt_2\n",
    "    def search_alt(self, alt_1 = 5, alt_2 = 8):\n",
    "        Portfolio_including = []\n",
    "        for elements in range(1, np.asarray(dataset.list_combinations).shape[0]):\n",
    "            if alt_1 in np.asarray(dataset.list_combinations)[elements] and alt_2 in np.asarray(dataset.list_combinations)[elements]:\n",
    "                Portfolio_including.append(elements)\n",
    "        return Portfolio_including\n",
    "    \n",
    "    def portfolio_value_dist(self, attribute = \"Attribute_1\", portfolio = 118):\n",
    "        self.portfolio_dist = pd.DataFrame(self.F.T[portfolio-1].T[self.info.columns.tolist().index(attribute)], columns = [\"Portfolio number: \" + str(portfolio)])\n",
    "        return self.portfolio_dist\n",
    "    \n",
    "    def portfolio_value_dist_all(self, portfolio = 118):\n",
    "        self.portfolio_dist_merged = pd.DataFrame()\n",
    "        for attribute in self.info.columns:\n",
    "            self.attribute_temp = attribute\n",
    "            self.portfolio_value_dist(attribute = attribute, portfolio = portfolio)\n",
    "            self.portfolio_dist_merged[attribute] = self.portfolio_dist[\"Portfolio number: \" + str(portfolio)].values\n",
    "            \n",
    "        return self.portfolio_dist_merged\n",
    "    \n",
    "    def boxplot_portfolio_values(self, attribute = \"Attribute_1\", portfolio = 118):\n",
    "        sns.set(rc={\"figure.figsize\":(4, 5)})\n",
    "        sns.set_style('whitegrid')\n",
    "        self.portfolio_test = pd.DataFrame(self.F.T[portfolio-1].T[self.info.columns.tolist().index(attribute)], columns = [\"Portfolio number: \" + str(portfolio)])\n",
    "        ax = sns.boxplot(x = 'variable', y = 'value', data = pd.melt(self.portfolio_test), showfliers = False, palette = \"Blues\")\n",
    "        ax.set_title(\"Impact on: \" + str(attribute))\n",
    "        ax.set_xlabel(None)\n",
    "        return\n",
    "    \n",
    "    def boxplot_portfolio_values_all(self, portfolio = 118):\n",
    "        sns.set(rc={\"figure.figsize\":(7.5, 6)})\n",
    "        sns.set_style('whitegrid')\n",
    "        self.portfolio_all = self.portfolio_value_dist_all(portfolio = portfolio)\n",
    "        ax = sns.boxplot(x = 'variable', y = 'value', data = pd.melt(self.portfolio_all), showfliers = False, palette = \"Blues\")\n",
    "        ax.set_title(\"Portfolio \" + str(portfolio) + \": Impact on all attributes\")\n",
    "        ax.set_xlabel(None)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=40, horizontalalignment='right')\n",
    "        plt.show()\n",
    "        return \n",
    "\n",
    "        \n",
    "    \n",
    "V = Values()\n",
    "V.xi()\n",
    "V.ranks_2(samples = 100000)\n",
    "V.acceptability_indeces()\n",
    "V.dict_df()\n",
    "#V.return_info_partial_values()\n",
    "#V.boxplot_portfolio_values(attribute = \"Attribute_5\", portfolio = 118)\n",
    "#V.info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.core(\"Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Core_Y = V.core_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Core_Y[\"Alternative\"] = Core_Y.index\n",
    "Core_Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Produce the Barplot this code is used to generate the countings of the barplots\n",
    "#Be AWARE that Core_Y first needs to be generated unto which you do the first merger. After this you can merge the Core_Merger with Core_X\n",
    "#Replace the old Core_Merger with the new one.\n",
    "def barplot_data(actor = \"X\"):\n",
    "    #The V.core(actor) runs the Value Class method so have the attribute ranking also right for the corresponding actor.\n",
    "    V.core(actor)\n",
    "    Core_temp = V.core_index\n",
    "    Core_temp[\"Alternative\"] = Core_temp.index\n",
    "    Core_Merged = Core_Y.merge(Core_temp, how = \"outer\")\n",
    "    \n",
    "    #After all actors are incorporated into the Core_Merged the alternative values are obtained from the weird tuple format:\n",
    "    Alternative_List = []\n",
    "    for i in Core_Merged[\"Alternative\"].values:\n",
    "        Alternative_List.append(i[0])\n",
    "    Core_Merged[\"Alternative\"] = Alternative_List\n",
    "    return Core_Merged\n",
    "\n",
    "#Plot the Barplot\n",
    "Core_merged = Core_merged.replace(np.nan,0)\n",
    "#Core_merged[\"Combined CI\"] = Core_merged[\"Actor Y\"]*Core_merged[\"Actor X\"]\n",
    "Core_merged.sort_index()\n",
    "sns.set(rc={\"figure.figsize\":(7, 5)})\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(data =Core_merged, x = \"Alternative\", y = \"Alternative Count\", hue = \"Actor\", palette = \"Blues\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot the Heatmap\n",
    "df_wide = Core_Merged.pivot_table(index='Alternative', columns='Actor', values='Core Index')\n",
    "rdgn = sns.diverging_palette(h_neg=10, h_pos=133, s=99, l=55, sep=3, as_cmap=True)\n",
    "sns.set(rc={\"figure.figsize\":(8, 5.5)})\n",
    "sns.heatmap(df_wide , center=0.5, cmap=rdgn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952a9b7",
   "metadata": {},
   "source": [
    "## Interaction Values\n",
    "The selection of best performing portfolios can be fine tuned by selecting from alreadybest ranking portfolios only portfolios with synergies. (So, highest Values with Synergy). In this second situation it is also assumed that the best performing portfolios have similar outcomes in which the existence of a synergy can decide in favour of very similar Portfolios.\n",
    "\n",
    "First all the alternative combinations with positive synergy values have to be loaded in.\n",
    "Secondly, the \"top\" performing portfolios are screened wheter these interactions do occur. This happens after a general MAUT ranking which already includes attribute preferences. Then the rankings are compared for the 5 best performing portfolios and the core projects in these portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction():\n",
    "    #dataset_1 is the original data from which the synergy combinations are extracted loaded into a panda dataframe.\n",
    "    #dataset_2 is the dataset of ranked portfolios in a panda dataframe.\n",
    "    def __init__(self, dataset_1 = Complete_Matrix, dataset_2 = Portfolio_Data.df_aggregated, top = V):\n",
    "        self.interaction_sets = dataset_1.interactions()\n",
    "        #here specify the top ranking portfolios to be included in the synergy check.\n",
    "        self.portfolios = dataset_2\n",
    "        self.top_ranking = top\n",
    "        self.top = np.unique(np.asarray(top.acceptability_indeces(10)))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def check(self):\n",
    "        #from the input of dataset_2 the portfolios are checked on the existence of synergies from the Excel File.\n",
    "        #These are than isolated from the other portfolios and returned in a dataframe.\n",
    "        self.portfolios_with_synergy = pd.DataFrame()\n",
    "        for i in range(1, len(Complete_Matrix.interactions())+1):\n",
    "            set_1 = Complete_Matrix.interactions().values[i-1][0], Complete_Matrix.interactions().values[i-1][1]\n",
    "            for j in self.top:\n",
    "                if set_1[0] in Portfolio_Data.list_combinations[j] and set_1[1] in Portfolio_Data.list_combinations[j]:\n",
    "                    #print(set_1[0], \"and\", set_1[1],\"are in portfolio\", str(j), \": \", Portfolio_Data.list_combinations[j])\n",
    "                    self.portfolios_with_synergy = self.portfolios_with_synergy.append(self.portfolios.iloc[[j-1]])   \n",
    "        return np.unique(self.portfolios_with_synergy[\"Strategic_Portfolio\"].values)\n",
    "    #to obtain only the strategic portfolio number add [[\"Strategic_Portfolio\"]].values to the above returned method.\n",
    "\n",
    "\n",
    "\n",
    "    def core(self, amount_of_ranks_included = 6, actor = \"Not Specified\"):\n",
    "        self.top = np.unique(np.asarray(self.top_ranking.acceptability_indeces(amount_of_ranks_included = amount_of_ranks_included)))\n",
    "        core_array = []\n",
    "        for i in self.check().flatten():\n",
    "            core_array.append(self.top_ranking.return_info_portfolio(i)[\"Alternative\"].to_list())\n",
    "        core_array_flat = list(it.chain(*core_array))\n",
    "        core_df = pd.DataFrame(core_array_flat, columns = [\"Core Alternative\"])\n",
    "        self.core_index = core_df.value_counts().to_frame(name = \"Alternative Count\")\n",
    "        #self.core_index[\"Actor\"] = actor\n",
    "        self.core_index.to_csv(\"C:/Users/paulu/Documents/Epa/Thesis/Portfolios \" + str(actor) + \".csv\")\n",
    "        return self.core_index\n",
    "\n",
    "Core_test = Interaction()\n",
    "Core = Core_test.core(2, actor = \"Y\")\n",
    "\n",
    "#Portfolios_with_Synergy = Interaction()\n",
    "#a = Portfolios_with_Synergy.check()\n",
    "#a = pd.DataFrame(a, columns = [\"Best Performing Portfolios for Actor Y\"])\n",
    "\n",
    "#Portfolios_with_Synergy.portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e207a",
   "metadata": {},
   "source": [
    "## Normalisation Techniques\n",
    "\n",
    "Mainly used for exploratory purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalisation():\n",
    "    def __init__(self,  dataset = Portfolio_Data):\n",
    "        self.inhereted_class = dataset\n",
    "        self.dataset = dataset.df_aggregated\n",
    "        self.x = self.dataset.values\n",
    "        #Indexes of the criteria that are deemed 'cost criteria'\n",
    "        self.cost = [6, 7, 8, 10]\n",
    "        self.z = \"Dataset not yet normalised\"\n",
    "        self.z_df = \"Dataframe not yet constructed\"\n",
    "        self.raise_norm = False\n",
    "        self.raise_mul = False\n",
    "        self.zw = \"Normalised Data not yet weighted, run weights_added() first\"\n",
    "        self.zw_df = \"Dataframe not yet constructed\"\n",
    "\n",
    "\n",
    "    def v_norm(self):\n",
    "        k = np.array(np.cumsum(self.x**2, axis=0))\n",
    "        #next step is to take the root of this numer k to obtain the magnitude of each column.\n",
    "        #Each element is divided by this magnitude to have it normalised.\n",
    "        #For row i and column j the elements are scaled by the corresponding magnitude ratio for that column.\n",
    "        #For example the first criteria, vector has a certain magnitude. \n",
    "        z = np.array([[round(self.x[i, j] / np.sqrt(k[self.x.shape[0] - 1, j]), 3) for j in range(self.x.shape[1])]\n",
    "        for i in range(self.x.shape[0])])\n",
    "        #print(\"The vector normalised matrix yields:\\n\")\n",
    "        self.z = z\n",
    "        self.raise_norm = True\n",
    "        return self.z\n",
    "    \n",
    "    def v_norm_df(self):\n",
    "        if self.raise_norm == False:\n",
    "            self.v_norm()\n",
    "        self.z_df = pd.DataFrame(self.z, columns = list(self.dataset.columns))\n",
    "        self.z_df[\"Strategic_Portfolio\"] = np.array(range(self.z.shape[0])) + 1\n",
    "        return self.z_df\n",
    "    \n",
    "    def norm_cost(self):\n",
    "        if self.raise_norm == False:\n",
    "            print(\"First run norm()\")\n",
    "            return\n",
    "        print('For criteria in indexes of self.cost, namely: ', self.cost, \" the values are transformed to cost criteria, altering the normalisation\")\n",
    "        for i in self.cost:\n",
    "                self.z[:, i:(i+1)] = 1 - self.z[:, i:(i+1)]\n",
    "        return self.z\n",
    "            \n",
    "        #For a criteria in the criteria column list, if it is one to be decreased, then the outcomes on that column have to substracted from one and replaced\n",
    "        #Or do this in the normalisation step to, with extra if statement, search via dataset.aggregate()\n",
    "        #T = 1 - self.z[:,0:1]\n",
    "        #self.z[:,0:1] = T\n",
    "        #return self.z\n",
    "\n",
    "      \n",
    "    def weights_added(self):\n",
    "        self.weights = self.inhereted_class.dataset.criteria_weights()[0]\n",
    "        if self.weights.shape[0] != self.z.shape[1]:\n",
    "            print(\"The amount of criteria do not match the amount of weights in the excel files; they have to be equal.\")\n",
    "            return\n",
    "        #weight= [range(self.z.shape[1])]\n",
    "        #self.w = np.ones(self.z.shape[1])\n",
    "        self.zw = np.array([[self.z[i, j] * self.weights[j]\n",
    "            for j in range(self.z.shape[1])]\n",
    "            for i in range(self.z.shape[0])])\n",
    "        #print(\"The weighted vector normalised matrix yields:\\n {}.\".format(self.zw))\n",
    "        self.raise_mul = True\n",
    "        return self.zw\n",
    "    \n",
    "    def weights_added_df(self):\n",
    "        if self.raise_mul == False:\n",
    "            self.weights_added()\n",
    "        self.zw_df = pd.DataFrame(self.zw, columns = list(self.dataset.columns))\n",
    "        self.zw_df[\"Strategic_Portfolio\"] = np.array(range(self.zw.shape[0])) + 1\n",
    "        return self.zw_df\n",
    "    \n",
    "    def run(self):\n",
    "        self.v_norm()\n",
    "        self.norm_cost()\n",
    "        self.v_norm_df()\n",
    "        self.weights_added()\n",
    "        self.weights_added_df()\n",
    "        print(\"The Portfolio Matrix has been normalised.\")\n",
    "        return\n",
    "\n",
    "Portfolio_Normalised = Normalisation()\n",
    "Portfolio_Normalised.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293eee52",
   "metadata": {},
   "source": [
    "## Topsis Implementation\n",
    "Used in exploratory first understanding of MCDA ranking techniques. Is deemed less adaquate than MAVT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60816720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topsis():\n",
    "    def __init__(self,  dataset = Portfolio_Normalised):\n",
    "        self.inhereted_class = dataset.inhereted_class\n",
    "        self.dataset = dataset.z\n",
    "        self.zw = dataset.zw\n",
    "        self.raise_zenith = False\n",
    "        self.normalisation = \"Not Specified\"\n",
    "        self.nadir_method = \"Not Specified\"\n",
    "   \n",
    "    def zenith_nadir(self):\n",
    "        \"\"\" zenith and nadir virtual action function; self.u is the\n",
    "        weighted normalized decision matrix and method is the\n",
    "        action used. For min/max input 'm' and for absolute\n",
    "        input enter 'a'\n",
    "        \"\"\"\n",
    "        u = self.zw\n",
    "        if self.nadir_method == 'm':\n",
    "                bb = []\n",
    "                cc = []\n",
    "                for i in range(u.shape[1]):\n",
    "                    bb.append(np.amax(u[:, i:i + 1]))\n",
    "                    b = np.array(bb)\n",
    "                    cc.append(np.amin(u[:, i:i + 1]))\n",
    "                    c = np.array(cc)\n",
    "                #print(\"The zenith is {} and the nadir is {}.\".format(b, c))\n",
    "                self.zenith = b\n",
    "                self.nadir = c\n",
    "                self.raise_zenith = True\n",
    "                return (self.zenith, self.nadir)\n",
    "        else:\n",
    "                #creates a vector of ones and zeros of length of matrix X\n",
    "                b = np.ones(u.shape[1])\n",
    "                print(b)\n",
    "                c = np.zeros(u.shape[1])\n",
    "                #print(\"The zenith is {} and the nadir is {}.\".format(b, c))\n",
    "                self.zenith = b\n",
    "                self.nadir = c\n",
    "                self.raise_zenith = True\n",
    "                return (self.zenith, self.nadir)     \n",
    "            \n",
    "            \n",
    "    def distance(self):\n",
    "        u = self.zw\n",
    "        \"\"\" calculate the distances to the ideal solution (di+)\n",
    "        and the anti-ideal solution (di-); u is the result\n",
    "        of mul_w() and b, c the results of zenith_nadir()\n",
    "        \"\"\"\n",
    "        distance = []            \n",
    "        ideal_i = []\n",
    "        non_ideal_i = []\n",
    "        for i in range(u.shape[0]):\n",
    "            #Alternatives\n",
    "            #The j is the amount of criteria.\n",
    "            g = 0\n",
    "            o = 0\n",
    "            for j in range(u.shape[1]):\n",
    "                #criteria\n",
    "                a = u[i, j] - self.zenith[j]\n",
    "                g += a**2\n",
    "                b = u[i, j] - self.nadir[j]\n",
    "                o += b**2\n",
    "            g = math.sqrt(g)\n",
    "            o = math.sqrt(o)\n",
    "            ideal_i.append(g)\n",
    "            non_ideal_i.append(o)\n",
    "        distance.append(g)\n",
    "        distance.append(o)\n",
    "        return np.asarray(ideal_i), np.asarray(non_ideal_i)\n",
    "            \n",
    "        return (np.sqrt(sum(a, 1)), np.sqrt(sum(b, 1)))\n",
    "        \n",
    "        #a = np.array([[(u[i, j] - self.b[j])**2 \n",
    "        #    for j in range(self.zw.shape[1])]\n",
    "        #    for i in range(self.zw.shape[0])])\n",
    "       # print(a)\n",
    "        #b = np.array([[(u[i, j] - self.c[j])**2 \n",
    "       #     for j in range(self.zw.shape[1])]\n",
    "       #     for i in range(self.zw.shape[0])])\n",
    "       # return (np.sqrt(sum(a, 1)), np.sqrt(sum(b, 1)))\n",
    "    \n",
    "    def single_portfolio(self, portfolio_set = 1):\n",
    "        self.portfolio_set = portfolio_set\n",
    "        return self.inhereted_class.single_portfolio(portfolio_set)\n",
    "    \n",
    "    def aggregated_single_portfolio(self, portfolio_set = 1):\n",
    "        return self.dataset.aggregated_single_portfolio(portfolio_set)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def topsis(self, pl = 'no', normalisation = \"v\", nadir_method = \"a\"):\n",
    "        self.normalisation = normalisation\n",
    "        self.nadir_method = nadir_method\n",
    "        if self.raise_zenith == False:\n",
    "            #print(\"First calculate zenith and nadir by running zenith_nadir()\")\n",
    "            self.zenith_nadir()\n",
    "        \"\"\" matrix is the initial decision matrix, weight is \n",
    "        the weights matrix, norm_m is the normalization \n",
    "        method, method is the action used by zenith_nadir(), and pl is 'yes' \n",
    "        for plotting the results or any other string for \n",
    "        not. \n",
    "        \n",
    "        \"\"\"\n",
    "        #z = self.zw\n",
    "        s, f = self.zenith, self.nadir\n",
    "\n",
    "        p, n = self.distance()\n",
    "        final_s = np.array([n[i] / (p[i] + n[i]) for i in range(p.shape[0])])\n",
    "        #Here merge the final_s outcomes with the original dataframe of the portfolios as to be able to select the best x performing portfolios.\n",
    "        #Now only the max is selected.\n",
    "        #C_F_list = pd.DataFrame(final_s, columns = [\"Closeness coefficient\"])\n",
    "        #self.list_total = self.dataset.join(C_F_list).sort_values(\"Closeness coefficient\", ascending = False)\n",
    "        \n",
    "        \n",
    "        if pl == 'yes':\n",
    "            q = [i + 1 for i in range(self.zw.shape[0])]\n",
    "            plt.plot(q, p, 'p--', color = 'red', \n",
    "                markeredgewidth = 1, markersize = 3)\n",
    "            plt.plot(q, n, '*--',  color = 'blue', \n",
    "                markeredgewidth = 1, markersize = 3)\n",
    "            plt.plot(q, final_s, 'o--', color = 'green',\n",
    "                markeredgewidth = 1, markersize = 3)\n",
    "            plt.title('TOPSIS results')\n",
    "            plt.legend(['Distance from the ideal', \n",
    "                'Distance from the anti-ideal', \n",
    "                'Closeness coefficient'])\n",
    "            #plt.xticks(range(self.zw.shape[0]+5))\n",
    "            if self.nadir_method == \"m\":\n",
    "                plt.axis([0, self.zw.shape[0] + 1, 0, 1.2])\n",
    "            else:\n",
    "                plt.axis([0, self.zw.shape[0] + 1, 0, 5])\n",
    "            plt.xlabel('Portfolios')\n",
    "            plt.grid(True)\n",
    "            plt.show\n",
    "\n",
    "\n",
    "            #Determine to which attribute the highest closseness coefficient corresponds:\n",
    "        place = np.where(final_s == final_s.max())[0]+1\n",
    "            #print(\"The place of the max attribute is \", place)\n",
    "        print(\"The maximum value of the closeness coefficients is: {}, which corresponds to portfolio {} from the decision matrix.\".format(final_s.max(), place))\n",
    "\n",
    "        self.raise_mul = False\n",
    "        self.raise_norm = False\n",
    "        self.raise_zenith = False\n",
    "        self.normalisation = \"Not Specified\"\n",
    "        self.nadir_method = \"Not Specified\"\n",
    "        #print(\"The closeness coefficients are: {}, with maximum value {}, which corresponds to portfolio {} from the decision matrix\".format(final_s.self, final_s.self.max(), place))\n",
    "        return self.single_portfolio(int(place))\n",
    "\n",
    "\n",
    "Topsis_Method = Topsis()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
